#============================#
#======= hldc  config =======#
#============================#

#elastic客户端地址 ,多个客户端用逗号分隔
elasticSearch.address=10.1.70.200:9300
elasticSearch.main.ip=10.1.70.200
elasticSearch.main.ip.desc=ElasticSearch 部署地址       
elasticSearch.port=9300
elasticSearch.port.desc=ElasticSearch 连接端口 
#elasticSearch.address=10.1.70.11:9300
#elastic 集群名称  见ES服务器配置
elasticSearch.cluster=elasticsearch
#elastic 数据对象索引 索引必须小写
elasticSearch.dataObj.index=dc_metadata
#elastic 数据对象类型 类型必须小写
#elasticSearch.dataObj.type=dc_datainfo
#type = table
elasticSearch.dataObj.tableType=table
#type = file
elasticSearch.dataObj.fileType=file
#type = field
elasticSearch.dataObj.fieldType=field
#type = interface
elasticSearch.dataObj.interType=interface
#type = folder
elasticSearch.dataObj.folderType=folder
#type = label
elasticSearch.dataObj.labelType=label
#elastic 批量提交设置
elasticSearch.bat.num=10000
#elastic 分页设置
elasticSearch.pager.num=20


#neo4j 配置
jersey.neo4j=http://10.1.20.130:10010/jerseyNeo4j/
 #10.1.21.54:8010/jerseyNeo4J

#sqoop client1 配置 (DB数据采集)
#sqoop.client.address=10.1.70.203,10.1.70.202,10.1.70.204
sqoop.client.address=10.1.70.200,10.1.70.201,10.1.70.202,10.1.70.203,10.1.70.204
sqoop.client.loginUser=root
sqoop.client.loginPswd=inteast.com

#hadoop yarn job监控端口(监控hadoop运行jod)
hadoop.main.address=10.1.70.200
hadoop.cluster.port=8088
hadoop.jobhistory.port=19888

#hdfs nameNode目录 
hdfs.namenode.server=10.1.70.200
#hdfs client节点 用于读写/查看节点内容/日志文件
hdfs.datanode.address=10.1.70.200
#hdfs 业务日志存储根目录
hdfs.bizlog.baseDir=/log/bizLog
#hdfs fs连接用户 , 用于创建hdfs文件 
hdfs.fs.connUser=hdfs
#hdfs 采集任务 文件备份目录
hdfs.extractJob.backupDir=/tmp/hl_bak/
#hive 采集/转换任务 数据表备份database
hive.extractJob.backupDB=hl_bak
#rdbms 关系数据库数据导出备份表 前缀
rdbms.export.baktable.prefix=dc_bak_


#hdfs文件采集任务 distcp命令配置
distcp.client.address=10.1.70.200
distcp.client.loginUser=root
distcp.client.loginPswd=inteast.com


#hue 集群配置
hue.server.address=10.1.70.200:8888


#hive 元数据连接配置
hive.metadata.connect=10.1.70.200:3306/hive
hive.metadata.user=hiveldc
hive.metadata.pswd=hive

#hive udf默认hdfs上的目录 与 hive的hive.aux.jars.path路径一致
hive.udf.path=/home/test/lp/jar/

#tos_db 数据清洗工具配置(服务端的)
#tool.taland.location=D://tools//TOS_BD-20160704_1411-V6.2.1//TOS_BD-win-x86_64.exe

#解析Sql restful服务 配置(服务器太卡, 目前用自己的电脑)
parseSql.restServer.url=http://10.1.70.87:9050/jerseyParseSql/rest/dcParseSql/

#查询hive-jdbc脚本 restful服务 配置(服务器太卡, 目前用自己的电脑)
#queryHive.restServer.url=http://10.1.20.87:9050/jerseyHive/rest/dcHiveQuery/queryMetaSql
queryHive.restServer.url=http://10.1.70.130:10010/jerseyHive/rest/dcHiveQuery/queryMetaSql
executeHive.restServer.url=http://10.1.70.130:10010/jerseyHive/rest/dcHiveQuery/executeMetaSql
#查询hive-jdbc脚本 默认返回结果数
queryHive.result.dataLimit=20

#hbase restful server 配置地址, 用于hbase数据预览
hbase.rest.server=http://10.1.70.138:20550
#hbase scanner 批处理数据量(每次scanner查询的数据量)
hbase.scanner.batch=100
#hbase restServer api and By peijd 2017-5-3
hbase.restServer.url=http://10.1.70.130:9060/hbaseapi/rest/mainaction
hbase.restServer.url.desc=hbase rest服务端 配置

#DB数据采集 数据预览显示列表数量
extractdb.preview.datanum=20


#cloudera manager 集群监控restful服务地址
clouderaManager.rest.api.url=http://10.1.70.130:10010/jerseyClusterMonitor

#Hive查询显示最大数量
queryHive.result.count=30

#接口数据查询  分页-每页默认数量
intf.default.pageSize=100
intf.default.pageSize.desc=接口数据分页查询,每页默认获取数量
intf.max.pageSize=200
intf.max.pageSize.desc=接口数据分页查询,每页最多获取数量


elasticSearch.address.desc=elastic客户端地址 ,多个客户端用逗号分隔
elasticSearch.cluster.desc=elastic 集群名称  见ES服务器配置
elasticSearch.dataObj.index.desc=elastic 数据对象索引 索引必须小写
elasticSearch.dataObj.tableType.desc=elastic 数据对象类型 table
elasticSearch.dataObj.fileType.desc=elastic 数据对象类型 file
elasticSearch.dataObj.fieldType.desc=elastic 数据对象类型 field
elasticSearch.dataObj.interType.desc=elastic 数据对象类型 interface
elasticSearch.dataObj.folderType.desc=elastic 数据对象类型 folder
elasticSearch.dataObj.labelType.desc=elastic 数据对象类型 label
elasticSearch.bat.num.desc=elastic 批量提交设置
elasticSearch.pager.num.desc=elastic 分页设置
jersey.neo4j.desc=neo4j 配置
sqoop.client.address.desc=sqoop client1 配置 (DB数据采集)
sqoop.client.loginUser.desc=sqoop client1 配置 (DB数据采集) 用户名
sqoop.client.loginPswd.desc=sqoop client1 配置 (DB数据采集) 密码
hadoop.main.address.desc=hadoop yarn job监控端口(监控hadoop运行jod)地址
hadoop.cluster.port.desc=hadoop yarn job监控端口(监控hadoop运行jod)
hadoop.jobhistory.port.desc=hadoop yarn job监控端口(监控hadoop运行jod)
hdfs.namenode.server.desc=hdfs nameNode目录
hdfs.datanode.address.desc=hdfs client节点 用于读写/查看节点内容/日志文件
hdfs.bizlog.baseDir.desc=hdfs 业务日志存储根目录
hdfs.fs.connUser.desc=hdfs fs连接用户 , 用于创建hdfs文件 
hdfs.extractJob.backupDir.desc=hdfs 采集任务 文件备份目录
hive.extractJob.backupDB.desc=hive 采集/转换任务 数据表备份database
rdbms.export.baktable.prefix.desc=rdbms 关系数据库数据导出备份表 前缀
distcp.client.address.desc=hdfs文件采集任务 distcp命令配置地址
distcp.client.loginUser.desc=hdfs文件采集任务 distcp命令配置用户名
distcp.client.loginPswd.desc=hdfs文件采集任务 distcp命令配置密码
hue.server.address.desc=hue 集群配置
hive.metadata.connect.desc=hive 元数据连接配置
hive.metadata.user.desc=hive 元数据连接配置用户名
hive.metadata.pswd.desc=hive 元数据连接配置密码
parseSql.restServer.url.desc=解析Sql restful服务 配置(服务器太卡, 目前用自己的电脑)
queryHive.restServer.url.desc=查询hive-jdbc脚本 restful服务 配置(服务器太卡, 目前用自己的电脑)
executeHive.restServer.url.desc=查询hive-jdbc脚本 restful服务 配置(服务器太卡, 目前用自己的电脑)
queryHive.result.dataLimit.desc=查询hive-jdbc脚本 默认返回结果数
hbase.rest.server.desc=hbase restful server 配置地址, 用于hbase数据预览
hbase.scanner.batch.desc = hbase scanner 批处理数据量(每次scanner查询的数据量)
extractdb.preview.datanum.desc=DB数据采集 数据预览显示列表数量
queryHive.result.count.desc=Hive查询显示最大数量
clouderaManager.rest.api.url.desc=cloudera manager 集群监控restful服务地址
hive.udf.path.desc=Hive UDF的jar在hdfs上的目录
